---
tags:
  - note
  - literature
  - 机器学习
  - 强化学习
  - 读书笔记
optional_tags:
  - fleeting
  - literature
  - permanent
  - iwhr
  - 引江济淮
  - 珠三角
  - 工作
  - 学习
aliases: 
duetime: 
created: 2024-08-07
completion: 
share: true
category:
  - essay
file_name: 2024-08-07-策略梯度读书笔记
---
参考 [论文分享 - 策略梯度算法专题 - 字舞流文](https://paperexplained.cn/articles/article/detail/31/#c96148d4-eed9-49f3-aea7-0a7563ea8bc9)  
  
**结论**：无偏策略梯度的形式为：  
  
$$  
\nabla_\theta J(\theta)  = \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)]  
$$  
## 定义  
  
|                           符号 | 含义                                                                                                                    |  
| ---------------------------: | --------------------------------------------------------------------------------------------------------------------- |  
|        $s\in \mathcal{S}$ ​​ | 状态                                                                                                                    |  
|           $a\in \mathcal{A}$ | 行为、动作 (actions)                                                                                                       |  
|           $r\in \mathcal{R}$ | 奖励 (rewards)                                                                                                          |  
|              $S_t, A_t, R_t$ | 一条样本采样/运行路径上时刻$t$ 时的状态、行为、奖励。文中有时也会使用$s_t, a_t, r_t$                                                                  |  
|                     $\gamma$ | 衰减系数；用于对未来不确定性的惩罚；$0 \lt \gamma \le 1$                                                                                |  
|                        $G_t$ | 收益；或带衰减的收益：$G_t = \sum_{k=0}^{\infty} \gamma^{k}R_{t+k+1}$                                                            |  
| $P(s^{\prime}, r\vert s, a)$ | 在状态$s$ 下执行行为$a$ 后，得到的奖励为$r$ ，并且进入状态$s^\prime$ 的转移概率                                                                   |  
|              $\pi(a\vert s)$ | 随机策略（执行代理(agent)的行为策略）；$\pi_{\theta}(\cdot)$ 是带参数$\theta$ 的策略                                                         |  
|                   $\mu(s)$ ​ | 确定性策略；我们也可以使用$\pi(s)$ ，但是使用符号$\mu$ 可以让我们更容易区分随机性策略和确定性策略。$\pi$ 和$\mu$ 都是在强化学习算法训练过程中学习的                               |  
|                       $V(s)$ | 状态价值函数、用于评估状态$s$ ​下的期望收益；$V_w(\cdot)$ 表示价值函数的参数为$w$ ​                                                                 |  
|                 $V^{\pi}(s)$ | 在服从策略$\pi$ 的情况下状态$s$ 的期望价值/收益                                                                                         |  
|                    $Q(s, a)$ | 行为价值函数，和$V(s)$ 类似，但是其评估的是状态和行为对$(s, a)$ 的期望收益；$Q_{w} (\cdot)$ 表示$Q$ 函数由参数$w$ 决定                                       |  
|             $Q^{\pi} (s, a)$ | 和$V^{\pi} (\cdot)$ ​​​​类似，但是$Q(s, a)$ ​​​​服从策略$\pi$ ​​​​；$Q^{\pi}(s, a) = E_{a \sim \pi}[G_t\vert S_t=s, A_t=a]$ ​​​​ |  
|                    $A(s, a)$ | 优势函数$A(s, a) = Q(s, a) - V(s)$ ；其可以被看作是另外一种形式的Q值函数，但其以值函数$V$ 作为基线，大大降低了方差(variance)                                   |  
|                              |                                                                                                                       |  
## 强化学习基本目标  
  
强化学习的最终目标是得到一套策略$\pi_\theta$使得收益$J(\theta)$最大化。收益被定义为  
$$  
J(\theta)   
= \sum_{s \in \mathcal{S}} d^\pi(s) V^\pi(s)   
= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a)  
$$  
其中$d^\pi(s)$为由$\pi_\theta$决定的马尔可夫平稳分布，即若策略$\pi_\theta$能够遍历马氏链中的所有状态，并且执行的时间足够久，最终停留在某个状态$s$的概率是一定的，这就是$d^{\pi}(s)=\lim _{t\to \infty} P(s_t=s|s_0,\pi_\theta)$。  
  
但对于连续状态空间而言，**遍历马氏链中的所有状态**是不可能的，因此$d^\pi(s)$事实上是不可解的。处于同样的理由，连续动作空间中的$Q^{\pi} (s, a)$事实上也是不可解的。对于离散状态空间或者动作空间，$\mathcal{S}$和$\mathcal{A}$的维度通常不会太小，极易陷入到维数灾难中。**这些解的复杂性大大提升了强化学习的解的难度**  
  
还好我们有策略梯度。  
  
策略梯度是$J(\theta)$的梯度$\nabla_\theta J(\theta)$，虽然$J(\theta)$本身无法直接求解，但策略梯度的值可以进行一定的转换和近似。  
  
## 策略梯度定理  
  
毫无疑问，对于一个稍微复杂一点的问题，$J(\theta)$是不可能有解析解的，其数值解也同样可以认为不可达。策略梯度定理对策略梯度的形式进行了一定的转换，在转换过程中假定$\nabla_\theta d^\pi \approx 0$，这个假定是出于：  
  
$$  
\begin{aligned}  
\nabla_\theta J(\theta)   
&= \nabla_\theta \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \pi_\theta(a \vert s) \\  
&\propto \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} Q^\pi(s, a) \nabla_\theta \pi_\theta(a \vert s)   
\end{aligned}  
$$  
  
其证明过程主要从价值函数开始：  
$$  
\begin{aligned}  
& \nabla_\theta V^\pi(s) \\  
=& \nabla_\theta \Big(\sum_{a \in \mathcal{A}} \pi_\theta(a \vert s)Q^\pi(s, a) \Big) & \\  
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta Q^\pi(s, a)} \Big) & \scriptstyle{\text{; 乘法求导法则}} \\  
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\nabla_\theta \sum_{s', r} P(s',r \vert s,a)(r + V^\pi(s'))} \Big) & \scriptstyle{\text{; 使用未来状态展开} Q^\pi } \\  
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s', r} P(s',r \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{P(s',r \vert s,a) \text{ 和 } r \text{ 不是 }\theta\text{的函数}}\\  
=& \sum_{a \in \mathcal{A}} \Big( \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) + \pi_\theta(a \vert s) \color{red}{\sum_{s'} P(s' \vert s,a) \nabla_\theta V^\pi(s')} \Big) & \scriptstyle{\text{; 因为 }  P(s' \vert s, a) = \sum_r P(s', r \vert s, a)}  
\end{aligned}  
$$  
在策略$\pi_\theta$下从状态$s$到状态$s_x$的k步转移概率记作$\rho^\pi(s \to s_x, k)$。当$k=1$时，$\rho^\pi(s \to s', k=1) = \sum_a \pi_\theta(a \vert s) P(s' \vert s, a)$。针对任意的$k$和$s_x$，可以先计算$k-1$步达到$s_{x'}$的概率$\rho^\pi(s \to s_{x'}, k-1)$，再算得$\rho^\pi(s \to s_x, k) = \sum_{s_{x'}} \rho^\pi(s \to s_{x'}, k-1) \rho^\pi(s_{x'} \to s_x, 1)$。  
  
将$\nabla_\theta V^\pi(s)$的前半段记作$\phi(s) = \sum_{a \in \mathcal{A}} \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a)$，则$\nabla_\theta V^\pi(s)$可以记作：  
  
$$  
\begin{aligned}  
& \color{red}{\nabla_\theta V^\pi(s)} \\  
=& \phi(s) + \sum_a \pi_\theta(a \vert s) \sum_{s'} P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\  
=& \phi(s) + \sum_{s'} \sum_a \pi_\theta(a \vert s) P(s' \vert s,a) \color{red}{\nabla_\theta V^\pi(s')} \\  
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{\nabla_\theta V^\pi(s')} \\  
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{\nabla_\theta V^\pi(s')} \\  
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \color{red}{[ \phi(s') + \sum_{s''} \rho^\pi(s' \to s'', 1) \nabla_\theta V^\pi(s'')]} \\  
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\color{red}{\nabla_\theta V^\pi(s'')} \scriptstyle{\text{ ; 考虑 }s'\text{ 为 }s \to s''} \text{ 的中间状态 }\\  
=& \phi(s) + \sum_{s'} \rho^\pi(s \to s', 1) \phi(s') + \sum_{s''} \rho^\pi(s \to s'', 2)\phi(s'') + \sum_{s'''} \rho^\pi(s \to s''', 3)\color{red}{\nabla_\theta V^\pi(s''')} \\  
=& \dots \scriptstyle{\text{; 持续展开 }\nabla_\theta V^\pi(.)} \\  
=& \sum_{x\in\mathcal{S}}\sum_{k=0}^\infty \rho^\pi(s \to x, k) \phi(x)  
\end{aligned}  
$$  
  
带回到$\nabla_\theta J(\theta)$中可以得到：  
$$  
\begin{aligned}  
\nabla_\theta J(\theta)  
&= \nabla_\theta V^\pi(s_0) & \scriptstyle{\text{; 初始状态设为 } s_0} \\  
&= \sum_{s}\color{blue}{\sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)} \phi(s) &\scriptstyle{\text{; Let }\color{blue}{\eta(s) = \sum_{k=0}^\infty \rho^\pi(s_0 \to s, k)}} \\  
&= \sum_{s}\eta(s) \phi(s) & \\  
&= \Big( {\sum_s \eta(s)} \Big)\sum_{s}\frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\text{; 归一化 } \eta(s), s\in\mathcal{S} \text{ 为一个概率分布}}\\  
&\propto \sum_s \frac{\eta(s)}{\sum_s \eta(s)} \phi(s) & \scriptstyle{\sum_s \eta(s)\text{  是一个常数}} \\  
&= \sum_s d^\pi(s) \sum_a \nabla_\theta \pi_\theta(a \vert s)Q^\pi(s, a) & \scriptstyle{d^\pi(s) = \frac{\eta(s)}{\sum_s \eta(s)}\text{ 是一个平稳分布}} \\  
&= \sum_{s \in \mathcal{S}} d^\pi(s) \sum_{a \in \mathcal{A}} \pi_\theta(a \vert s) Q^\pi(s, a) \frac{\nabla_\theta \pi_\theta(a \vert s)}{\pi_\theta(a \vert s)} &\\  
&= \mathbb{E}_\pi [Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)] & \scriptstyle{\text{; 因为 } (\ln x)' = 1/x}  
\end{aligned}  
$$  
问题转变为如何求取$Q^\pi(s, a) \nabla_\theta \ln \pi_\theta(a \vert s)$在$s\sim d_\pi, a\sim \pi_\theta$下的期望。  
  
## 常见策略梯度形式  
  
### 基础形式  
  
$$  
g = \mathbb{E}\left[ \sum_{t=0}^\infty Q^\pi(s, a) \nabla_\theta log\pi_\theta(a_t|s_t) \right],  
$$  
  
### 改进形式  
  
$$  
g = \mathbb{E}\left[ \sum_{t=0}^\infty \Psi_t \nabla_\theta log\pi_\theta(a_t|s_t) \right],  
$$  
  
其中 $\Psi_t$的常见的取值包括：  
  
1. 总奖励：$\sum_{t^\prime=t}^{\infty}r_{t^{\prime}}$；  
2. $t$时间之后的奖励：$\sum_{t^\prime=t}^{\infty}r_{t^{\prime}}$；  
3. 有baseline的总奖励：$\sum_{t=0}^{\infty}r_t - b(s_t)$；  
4. 优势函数：$A^\pi (s_t, a_t) := Q^\pi(s_t, a_t) - V^\pi(s_t, a_t)$；  
5. TD残差：$r_t + V^\pi(s_{t+1}) - V^{\pi}(s_t)$。  
其中，$V^\pi(s_t) := \mathbb{E}_{s_{t+1:\infty}, a_{t:\infty}} \sum_{l=0}^{\infty}r_{t+l}$。  
